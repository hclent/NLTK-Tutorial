{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from more_itertools import unique_everseen\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but \n",
    "with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. \n",
    "We'll begin by looking at synonyms and how they are accessed in WordNet.'''\n",
    "\n",
    "print(wn.synsets('car'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('car'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Navigate between concepts with hyponyms'''\n",
    "car = wn.synset('car.n.01')\n",
    "types_of_cars = car.hyponyms()\n",
    "print(sorted(lemma.name() for synset in types_of_cars for lemma in synset.lemmas()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''You can also navigate up the hierarchy by visiting hypernyms. \n",
    "Some words have multiple paths, because they can be classified in more than one way. \n",
    "There are two paths between car.n.01 and entity.n.01 because \n",
    "wheeled_vehicle.n.01 can be classified as both a vehicle and a container.'''\n",
    "\n",
    "print(car.hypernyms())\n",
    "paths = car.hypernym_paths()\n",
    "len(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([synset.name() for synset in paths[0]])\n",
    "print('---------------------------------------')\n",
    "print([synset.name() for synset in paths[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''We can get the most general hypernyms (or root hypernyms) of a synset as follows'''\n",
    "car.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''More Lexical Relations'''\n",
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Meronyms\n",
    "wn.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Holonyms\n",
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('mint', wn.NOUN):\n",
    "    print(synset.name() + ':', synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(wn.synset('mint.n.04').part_holonyms())\n",
    "print(wn.synset('mint.n.04').substance_holonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Entailment\n",
    "print(wn.synset('walk.v.01').entailments())\n",
    "#print(wn.synset('eat.v.01').entailments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Antonyms\n",
    "print(wn.lemma('supply.n.02.supply').antonyms())\n",
    "print(wn.lemma('rush.v.01.rush').antonyms())\n",
    "print(wn.lemma('horizontal.a.01.horizontal').antonyms())\n",
    "print(wn.lemma('staccato.r.01.staccato').antonyms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Semantic Similarity'''\n",
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')\n",
    "print(\"Minke hypernym:\")\n",
    "print(right.lowest_common_hypernyms(minke))\n",
    "print(\"Orca: hypernym: \")\n",
    "print(right.lowest_common_hypernyms(orca))\n",
    "print(\"Tortoise hypernym: \")\n",
    "print(right.lowest_common_hypernyms(tortoise))\n",
    "print(\"Novel hypernym: \")\n",
    "print(right.lowest_common_hypernyms(novel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#synset depth\n",
    "wn.synset('baleen_whale.n.01').min_depth()\n",
    "\n",
    "#wn.synset('whale.n.02').min_depth()\n",
    "\n",
    "#wn.synset('vertebrate.n.01').min_depth()\n",
    "\n",
    "#wn.synset('entity.n.01').min_depth()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Similarity Scores'''\n",
    "right.path_similarity(minke)\n",
    "\n",
    "# right.path_similarity(orca)\n",
    "\n",
    "# right.path_similarity(tortoise)\n",
    "\n",
    "# right.path_similarity(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Example: Using WordNet to find organisms in biology texts'''\n",
    "\n",
    "senses = (wn.synsets('plant', pos=\"n\"))\n",
    "print(senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = senses[0]\n",
    "s2 = senses[1]\n",
    "s3 = senses[2]\n",
    "s4 = senses[3]\n",
    "d1 = (s1.definition()) \n",
    "print(\"Def 1: \" + str(d1))\n",
    "d2 = (s2.definition())\n",
    "print(\"Def 2: \" + str(d2))\n",
    "d3 = (s3.definition())\n",
    "print(\"Def 3: \" + str(d3))\n",
    "d4 = (s4.definition()) \n",
    "print(\"Def 4: \" + str(d4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load text\n",
    "def readMe(file):\n",
    "    raw_text = open(file, 'r')\n",
    "    raw_text = raw_text.read()\n",
    "    return raw_text\n",
    "\n",
    "text = readMe(\"/Users/hclent/Desktop/NLPtutorial/texts/18952863_4.txt\")\n",
    "#print(fcorpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatCorpus(corpus): #as string\n",
    "    corpus = corpus.replace(\"_\", \"underscore\") #underscores will make problems for buildDict and are unimportant for my NER\n",
    "    #make untagged corpus\n",
    "    untagged_corpus = corpus.lower()\n",
    "    #untagged_corpus = untagged_corpus.split() #tokenize\n",
    "    untagged_corpus = nltk.word_tokenize(untagged_corpus)\n",
    "    stopwords = nltk.corpus.stopwords.words('english') #delete stopwords from untagged\n",
    "    untagged_corpus = [w for w in untagged_corpus if w.lower() not in stopwords]\n",
    "\n",
    "\n",
    "    return untagged_corpus\n",
    "\n",
    "untagged_corpus = formatCorpus(text)\n",
    "#print(untagged_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use WordNet to find organisms\n",
    "def wordNetNER(document):\n",
    "    plant_sns = (wn.synsets('plant', pos=\"n\"))\n",
    "    plant = plant_sns[1] #(botany) a living organism lacking the power of locomotion #hardcoded\n",
    "\n",
    "    wordnet_names = []\n",
    "    \n",
    "    for word in document:\n",
    "\n",
    "        mySynsets = wn.synsets(word, pos=\"n\") #look at nouns\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        for i in range(0, 3):\n",
    "            try:\n",
    "                given_word = mySynsets[i] #tries first 3 synsets\n",
    "                definition = (given_word.definition())\n",
    "                p1 = re.compile('plant(s?)\\s')\n",
    "                p2 = re.compile('organism(s?)\\s')\n",
    "                p3 = re.compile('animal(s?)\\s')\n",
    "                match1 = p1.search(definition)\n",
    "                match2 = p2.search(definition)\n",
    "                match3 = p3.search(definition)\n",
    "\n",
    "                if match1 or match2 or match3:  #if word has \"plants\" or \"animals\" in the def, check how similar\"\n",
    "                    similarity_score = (given_word.path_similarity(plant)) #check similarity score\n",
    "                    if similarity_score >= 0.2:\n",
    "                        #print(similarity_score)\n",
    "                        #print (\"The words: \"+(str(given_word)) + \"  has a sim score of:  \" +str(similarity_score))\n",
    "                        wordnet_names.append(word)\n",
    "                i += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    wordnet_ner = (list(unique_everseen(wordnet_names)))\n",
    "    return wordnet_ner\n",
    "\n",
    "wordnet_names = wordNetNER(untagged_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"WORDNET: \")\n",
    "print(wordnet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pyNLP]",
   "language": "python",
   "name": "conda-env-pyNLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
