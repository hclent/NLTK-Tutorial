{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1176896\n",
      "The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "print(type(raw))\n",
    "print(len(raw))\n",
    "print(raw[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "254352\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
     ]
    }
   ],
   "source": [
    "'''Tokenize'''\n",
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('On', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('exceptionally', 'RB'),\n",
       " ('hot', 'JJ'),\n",
       " ('evening', 'NN'),\n",
       " ('early', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('July', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('young', 'JJ'),\n",
       " ('man', 'NN'),\n",
       " ('came', 'VBD'),\n",
       " ('out', 'RP'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('garret', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('he', 'PRP'),\n",
       " ('lodged', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('S.', 'NNP'),\n",
       " ('Place', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('walked', 'VBD'),\n",
       " ('slowly', 'RB'),\n",
       " (',', ','),\n",
       " ('as', 'IN'),\n",
       " ('though', 'IN'),\n",
       " ('in', 'IN'),\n",
       " ('hesitation', 'NN'),\n",
       " (',', ','),\n",
       " ('towards', 'NNS'),\n",
       " ('K.', 'NNP'),\n",
       " ('bridge', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens[1026:1062])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exceptionally', 'hot', 'evening', 'early', 'July', 'young', 'man', 'came', 'garret', 'lodged', 'S.', 'Place', 'walked', 'slowly', ',', 'though', 'hesitation', ',', 'towards', 'K.', 'bridge']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english') #delete stopwords from data\n",
    "clean_tokens = [w for w in tokens if w.lower() not in stopwords]\n",
    "print(clean_tokens[586:607])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'came'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sent = clean_tokens[586:607]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize(first_sent[7]) #'came'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(first_sent[7], pos=\"v\") #'came'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('/Users/hclent/Desktop/NLTK-tutorial/texts/document.txt', 'r')\n",
    "for line in f:\n",
    "    sentence = line.strip()\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pyNLP]",
   "language": "python",
   "name": "conda-env-pyNLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
